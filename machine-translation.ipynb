{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pickle\nimport random\nimport numpy as np","metadata":{"id":"RTLa0ONw8wZu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = pickle.load(open('../input/translation/DS_5_train_input','rb'))\noutput_text = pickle.load(open('../input/translation/DS_5_train_output','rb'))","metadata":{"id":"KgNTlrFY-4wS","execution":{"iopub.status.busy":"2022-04-26T04:21:05.708565Z","iopub.execute_input":"2022-04-26T04:21:05.708848Z","iopub.status.idle":"2022-04-26T04:21:05.720552Z","shell.execute_reply.started":"2022-04-26T04:21:05.708810Z","shell.execute_reply":"2022-04-26T04:21:05.719701Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"text_pairs = []\nfor input,output in zip(input_text,output_text):\n    output_lang = \"[start] \" + output + \"[end]\"\n    text_pairs.append((input, output_lang))","metadata":{"id":"sgdqON_yIPuV","execution":{"iopub.status.busy":"2022-04-26T04:22:50.170724Z","iopub.execute_input":"2022-04-26T04:22:50.171547Z","iopub.status.idle":"2022-04-26T04:22:50.181311Z","shell.execute_reply.started":"2022-04-26T04:22:50.171508Z","shell.execute_reply":"2022-04-26T04:22:50.180241Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(random.choice(text_pairs))","metadata":{"id":"ap2W79JKJWso","outputId":"3819ccc0-506d-49f4-b95d-e081a1b85ea5","execution":{"iopub.status.busy":"2022-04-26T04:22:53.139454Z","iopub.execute_input":"2022-04-26T04:22:53.139720Z","iopub.status.idle":"2022-04-26T04:22:53.146051Z","shell.execute_reply.started":"2022-04-26T04:22:53.139691Z","shell.execute_reply":"2022-04-26T04:22:53.144925Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"random.shuffle(text_pairs)\nnum_val_samples = 995\nnum_test_samples = 5\nnum_train_samples = len(text_pairs) - num_val_samples - num_test_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples:]","metadata":{"id":"Ugba6gPaJeEf","execution":{"iopub.status.busy":"2022-04-26T04:22:57.897683Z","iopub.execute_input":"2022-04-26T04:22:57.898390Z","iopub.status.idle":"2022-04-26T04:22:57.910838Z","shell.execute_reply.started":"2022-04-26T04:22:57.898342Z","shell.execute_reply":"2022-04-26T04:22:57.910050Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(f\"{len(text_pairs)} total pairs\")\nprint(f\"{len(train_pairs)} training pairs\")\nprint(f\"{len(val_pairs)} validation pairs\")\nprint(f\"{len(test_pairs)} test pairs\")","metadata":{"id":"N8uLVLR2f3TM","outputId":"c5073395-7cef-4439-c6f7-db27fba3e7ae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 50\nsequence_length = 213\n\nsource_vectorization = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length\n)\ntarget_vectorization = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1\n)\ntrain_input_texts = [pair[0] for pair in train_pairs]\ntrain_output_texts = [pair[1] for pair in train_pairs]\nsource_vectorization.adapt(train_input_texts)\ntarget_vectorization.adapt(train_output_texts)","metadata":{"id":"3nWH63rOfuPw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n\ndef format_dataset(inp, out):\n    inp = source_vectorization(inp)\n    out = target_vectorization(out)\n    return ({\n        \"input_lang\": inp,\n        \"output_lang\": out[:, :-1],\n    }, out[:, 1:])\n\ndef make_dataset(pairs):\n    inp_texts, out_texts = zip(*pairs)\n    inp_texts = list(inp_texts)\n    out_texts = list(out_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((inp_texts, out_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n    return dataset.shuffle(2048).prefetch(16).cache()\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)","metadata":{"id":"tiNr29bFgt0f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for inputs, targets in train_ds.take(1):\n    print(f\"inputs['input_lang'].shape: {inputs['input_lang'].shape}\")\n    print(f\"inputs['output_lang'].shape: {inputs['output_lang'].shape}\")\n    print(f\"targets.shape: {targets.shape}\")","metadata":{"id":"B95wsBKchMo_","outputId":"ca4676e2-79a1-4f4f-a99b-205d92c3c5a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"),\n             layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            mask = mask[:, tf.newaxis, :]\n        attention_output = self.attention(\n            inputs, inputs, attention_mask=mask)\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim,\n        })\n        return config","metadata":{"id":"RkPlzNjJkNRF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"),\n             layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim,\n        })\n        return config\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1),\n             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n        return tf.tile(mask, mult)\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(\n                mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=causal_mask)\n        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n        attention_output_2 = self.attention_2(\n            query=attention_output_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n        )\n        attention_output_2 = self.layernorm_2(\n            attention_output_1 + attention_output_2)\n        proj_output = self.dense_proj(attention_output_2)\n        return self.layernorm_3(attention_output_2 + proj_output)","metadata":{"id":"KU7tM5RlhtAO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=input_dim, output_dim=output_dim)\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=output_dim)\n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n    def get_config(self):\n        config = super(PositionalEmbedding, self).get_config()\n        config.update({\n            \"output_dim\": self.output_dim,\n            \"sequence_length\": self.sequence_length,\n            \"input_dim\": self.input_dim,\n        })\n        return config","metadata":{"id":"UM0bl7AviVPi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 128\ndense_dim = 512\nnum_heads = 8\n\nencoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_lang\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\nencoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n\ndecoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"output_lang\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\nx = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\nx = layers.Dropout(0.5)(x)\ndecoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\ntransformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"id":"ZCWrYa55ihpN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.compile(\n    optimizer=\"rmsprop\",\n    loss=\"sparse_categorical_crossentropy\",\n    # metrics=['accuracy', acc_pred])\n    metrics=['accuracy'])\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        filepath=\"./translator_model.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\",\n        mode='min'),\n    keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        verbose=0,\n        mode='min'\n    )\n]\n\ntransformer.fit(train_ds, epochs=50, validation_data=val_ds, callbacks = callbacks)","metadata":{"id":"U7uF9qvOkZoD","outputId":"0715e7b1-e3c8-4635-b55b-601e1e6d5c54","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def acc_pred(y_true,y_pred):\n    total_acc_words = 0\n    total_num_words = 0\n    for true,pred in zip(y_true,y_pred):\n        true_words = true.split(\" \")\n        pred_words = pred.split(\" \")\n        count=0\n        length = min(len(true_words),len(pred_words))\n        for i in range(length):\n            if (true_words[i] == pred_words[i]):\n                count+=1\n        total_acc_words+= count\n        total_num_words+= len(true_words)\n    acc = np.float(total_acc_words)/(total_num_words)\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp_vocab = target_vectorization.get_vocabulary()\ninp_index_lookup = dict(zip(range(len(inp_vocab)), inp_vocab))\nmax_decoded_sentence_length = 211\n\nmodel = keras.models.load_model('translator_model.keras', custom_objects={'PositionalEmbedding':PositionalEmbedding, \"TransformerEncoder\": TransformerEncoder, \"TransformerDecoder\": TransformerDecoder})\n\ndef decode_sequence(input_sentence):\n    tokenized_input_sentence = source_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = target_vectorization(\n            [decoded_sentence])[:, :-1]\n        predictions = model(\n            [tokenized_input_sentence, tokenized_target_sentence])\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = inp_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"end\":\n            break\n            \n    decoded_sentence = decoded_sentence.replace(\"[start] \",\"\")\n    decoded_sentence = decoded_sentence.replace(\" end\",\"\")\n    return decoded_sentence\n\ntest_inp_texts = [pair[0] for pair in test_pairs]\ntest_out_texts = [pair[1] for pair in test_pairs]\ntest_pred_texts = [decode_sequence(input) for input in test_inp_texts]\n\naccuracy = acc_pred(test_out_texts,test_pred_texts)\nprint(f\"Test accuracy: {accuracy:.3f}\")\n\nprint(f\"Ground Truth: {test_out_texts[3]}\")\nprint(f\"Predicted: {test_pred_texts[3]}\")","metadata":{"id":"QBJNzo0FqAs4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instructions to run the model:","metadata":{}},{"cell_type":"code","source":"# Step 1\n# You would need to provide the path for 'DS_5_train_input' and 'DS_5_train_output' at mentioned places\n# Then run the entire cell\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pick\nimport random\nimport numpy as np\n\ninput_text = pickle.load(open(path_for_DS_5_train_input,'rb')) # path for 'DS_5_train_input'\noutput_text = pickle.load(open(path_for_DS_5_train_output,'rb')) # path for 'DS_5_test_input'\n\ntext_pairs = []\nfor input,output in zip(input_text,output_text):\n    output_lang = \"[start] \" + output + \"[end]\"\n    text_pairs.append((input, output_lang))\n    \nrandom.shuffle(text_pairs)\n\nvocab_size = 50\nsequence_length = 213\nbatch_size = 64\n\nsource_vectorization = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length\n)\ntarget_vectorization = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1\n)\n\ninput_texts = [pair[0] for pair in text_pairs]\noutput_texts = [pair[1] for pair in text_pairs]\nsource_vectorization.adapt(input_texts)\ntarget_vectorization.adapt(output_texts)\n\nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"),\n             layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            mask = mask[:, tf.newaxis, :]\n        attention_output = self.attention(\n            inputs, inputs, attention_mask=mask)\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim,\n        })\n        return config\n    \nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"),\n             layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim,\n        })\n        return config\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1),\n             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n        return tf.tile(mask, mult)\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(\n                mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=causal_mask)\n        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n        attention_output_2 = self.attention_2(\n            query=attention_output_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n        )\n        attention_output_2 = self.layernorm_2(\n            attention_output_1 + attention_output_2)\n        proj_output = self.dense_proj(attention_output_2)\n        return self.layernorm_3(attention_output_2 + proj_output)\n    \nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=input_dim, output_dim=output_dim)\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=output_dim)\n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n    def get_config(self):\n        config = super(PositionalEmbedding, self).get_config()\n        config.update({\n            \"output_dim\": self.output_dim,\n            \"sequence_length\": self.sequence_length,\n            \"input_dim\": self.input_dim,\n        })\n        return config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2\n# You would need to provide the path for 'DS_5_test_input' and 'DS_5_test_output' at mentioned places\n# You would need to provide the path for the model at the mentioned place\n\ninput_text = pickle.load(open(path_for_DS_5_test_input,'rb')) # path for 'DS_5_test_input'\noutput_text = pickle.load(open(path_for_DS_5_test_output,'rb')) # path for 'DS_5_test_output'\n\ntest_pairs = []\nfor input,output in zip(input_text,output_text):\n    output_lang = \"[start] \" + output + \"[end]\"\n    test_pairs.append((input, output_lang))\n    \ninp_vocab = target_vectorization.get_vocabulary()\ninp_index_lookup = dict(zip(range(len(inp_vocab)), inp_vocab))\nmax_decoded_sentence_length = 211\n\nmodel = keras.models.load_model(path_for_model, \n        custom_objects={'PositionalEmbedding':PositionalEmbedding, \n                        \"TransformerEncoder\": TransformerEncoder, \n                        \"TransformerDecoder\": TransformerDecoder}) # path for model\n\ndef acc_pred(y_true,y_pred):\n    total_acc_words = 0\n    total_num_words = 0\n    for true,pred in zip(y_true,y_pred):\n        true_words = true.split(\" \")\n        pred_words = pred.split(\" \")\n        count=0\n        length = min(len(true_words),len(pred_words))\n        for i in range(length):\n            if (true_words[i] == pred_words[i]):\n                count+=1\n        total_acc_words+= count\n        total_num_words+= len(true_words)\n    acc = np.float(total_acc_words)/(total_num_words)\n    return acc\n\ndef decode_sequence(input_sentence):\n    tokenized_input_sentence = source_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = target_vectorization(\n            [decoded_sentence])[:, :-1]\n        predictions = model(\n            [tokenized_input_sentence, tokenized_target_sentence])\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = inp_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"end\":\n            break\n            \n    decoded_sentence = decoded_sentence.replace(\"[start] \",\"\")\n    decoded_sentence = decoded_sentence.replace(\" end\",\"\")\n    return decoded_sentence\n\ntest_inp_texts = [pair[0] for pair in test_pairs]\ntest_out_texts = [pair[1] for pair in test_pairs]\ntest_pred_texts = [decode_sequence(input) for input in test_inp_texts]\n\naccuracy = acc_pred(test_out_texts,test_pred_texts)\nprint(f\"Test accuracy: {accuracy:.3f}\")\n\nprint(f\"Ground Truth: {test_out_texts[3]}\")\nprint(f\"Predicted: {test_pred_texts[3]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}